{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d45aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572627c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(state, bins):\n",
    "    state_bins = [np.digitize(state[i], bins[i]) - 1 for i in range(len(state))]\n",
    "    state_bins = [max(0, min(len(bins[i])-1, state_bins[i])) for i in range(len(state_bins))]\n",
    "    return tuple(state_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7339e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(num_bins):\n",
    "    position_bins = np.linspace(-1.2, 0.6, num_bins)\n",
    "    velocity_bins = np.linspace(-0.07, 0.07, num_bins)\n",
    "    return [position_bins, velocity_bins]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a79506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(num_bins, num_actions):\n",
    "    return np.zeros([num_bins, num_bins, num_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68377ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy(bins, num_actions):\n",
    "    policy = {}\n",
    "    for pos in range(len(bins[0])):\n",
    "        for vel in range(len(bins[1])):\n",
    "            policy[(pos, vel)] = np.random.choice(num_actions)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208f2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, q_table, policy, bins, gamma, theta, alpha):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for pos in range(len(bins[0])):\n",
    "            for vel in range(len(bins[1])):\n",
    "                state = (pos, vel)\n",
    "                action = policy[state]\n",
    "                old_value = q_table[state + (action,)]\n",
    "                env.reset()\n",
    "                env.unwrapped.state = (bins[0][pos], bins[1][vel])  # Set to the discrete state\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_state = discretize(next_state, bins)\n",
    "                done = terminated or truncated\n",
    "                if done:\n",
    "                    new_value = reward\n",
    "                else:\n",
    "                    new_value = reward + gamma * np.max(q_table[next_state])\n",
    "                q_table[state + (action,)] = (1 - alpha) * old_value + alpha * new_value\n",
    "                delta = max(delta, np.abs(old_value - q_table[state + (action,)]))\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc41770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(q_table, policy, bins):\n",
    "    policy_stable = True\n",
    "    for pos in range(len(bins[0])):\n",
    "        for vel in range(len(bins[1])):\n",
    "            state = (pos, vel)\n",
    "            old_action = policy[state]\n",
    "            policy[state] = np.argmax(q_table[state])\n",
    "            if old_action != policy[state]:\n",
    "                policy_stable = False\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a66046c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, q_table, bins, gamma, theta, episodes, alpha):\n",
    "    policy = initialize_policy(bins, env.action_space.n)\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        policy_evaluation(env, q_table, policy, bins, gamma, theta, alpha)\n",
    "        policy_stable = policy_improvement(q_table, policy, bins)\n",
    "        \n",
    "        state = discretize(env.reset()[0], bins)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = discretize(next_state, bins)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return rewards, q_table, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1b6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(rewards, granularity, parameter, value, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(rewards)), rewards, label=f'Granularity: {granularity}, {parameter}: {value}')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title(f'Reward vs Episodes ({parameter}: {value})')\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b9ce5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid map of policy\n",
    "def plot_policy(policy, bins, filename):\n",
    "    policy_grid = np.zeros((len(bins[0]), len(bins[1])))\n",
    "    for pos in range(len(bins[0])):\n",
    "        for vel in range(len(bins[1])):\n",
    "            policy_grid[pos, vel] = policy[(pos, vel)]\n",
    "    plt.figure()\n",
    "    plt.imshow(policy_grid, origin='lower', cmap='viridis')\n",
    "    plt.colorbar(ticks=[0, 1, 2], label='Action')\n",
    "    plt.title('Policy Grid')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030de5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "num_actions = env.action_space.n\n",
    "episodes = 5000\n",
    "gamma = 0.99\n",
    "theta = 0.0001  # Convergence threshold\n",
    "\n",
    "granularities = [ 20, 30]\n",
    "parameter = 'learning_rate'\n",
    "values = [0.1, 0.5, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for granularity in granularities:\n",
    "    for value in values:\n",
    "        bins = create_bins(granularity)\n",
    "        q_table = initialize_q_table(granularity, num_actions)\n",
    "        rewards, q_table, policy = policy_iteration(env, q_table, bins, gamma, theta, episodes, value)\n",
    "        plot_results(rewards, granularity, parameter, value, f'rewards_granularity_{granularity}_lr_{value}.png')\n",
    "        if granularity == 20:\n",
    "            plot_policy(policy, bins, f'policy_grid_granularity_{granularity}_lr_{value}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0449b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
